Overall Concept
1. Root Node (User Prompt)
    * The user begins by typing a prompt (e.g., “Design the interface for a 1000-floor elevator”).
    * This text becomes the root node on the Main Canvas, serving as the trunk of a tree from which all subsequent branches of questions extend.
2. AI-Powered Question Generation (via gpt-4-0125-preview)
    * Once the user sets their prompt and initial preferences (e.g., conflict handling), the AI dynamically generates follow-up questions.
    * It references previously provided answers and any uploaded knowledge base documents to decide which child nodes (sub-branches) to create.
    * Early questions are intentionally basic (e.g., “Is the elevator primarily for humans or machinery?”), helping the user refine fundamental requirements.
3. Breadth-First Q&A
    * The user first addresses all top-level questions (Q1, Q2, Q3, etc.), which appear as branches directly stemming from the root node.
    * Only after these siblings are answered does the system proceed to each branch’s child nodes (Q4, Q5, etc.), visually adding deeper sub-branches to the tree.
    * Depending on the user’s input, the AI might generate new branches, prune no longer relevant paths, or hide conflicting nodes automatically (if minor conflicts are set to auto-resolve).
4. Stopping Criteria & Generation
    * The user may cap the process at a certain number of questions (e.g., 20), prompting the system to offer a mockup at that point.
    * Alternatively, the AI can decide when enough information has been gathered.
    * Once the Q&A phase ends, the system compiles a requirements document and produces React/Tailwind code for a working mockup, reflecting all branches explored in the tree.

Step 1: Initial Setup & Prompt Node
1. Enter the Prompt:
    * The user types: “Design the interface for a 1000-floor elevator.”
    * This becomes the root node on the Main Canvas (labeled “Prompt: 1000-floor elevator”).
2. Optional Knowledge Base Upload (AI-Enhanced Auto-Population):
    * If the user uploads relevant documents (elevator guidelines, building specs, etc.), the AI may auto-populate answers to certain questions.
    * If no knowledge base is uploaded, no auto-populate option appears.
3. Unknown & Conflict Handling (AI Settings):
    * Unknowns: Choose “Auto for trivial unknowns” or “Always prompt.” (Trivial unknowns can be guessed by the AI if set.)
    * Conflicts: Choose “Auto-resolve minor conflicts” or “Manual resolution every time.”
        * If auto-resolve is on, the AI attempts to fix minor inconsistencies behind the scenes.
        * Major conflicts always prompt the user for input.
4. Question Limit or AI-Determined Stop:
    * Specify “Stop after X questions,” or
    * Let the AI judge when sufficient requirements are gathered for a design concept.
After these settings, the root node is finalized on the Main Canvas.

Step 2: Interactive Q&A Flow (AI-Driven)
2.1. The Root Node & First Questions
    * Main Canvas: Shows the root node (the user’s prompt).
    * Right-Hand Q&A Panel: A “Begin Q&A” button triggers the AI (gpt-4-0125-preview).
    * AI Question Generation:
        * The AI checks the prompt and any knowledge base data, then creates the first top-level questions.
        * Example initial questions (Q1, Q2, Q3):
            * Q1: “Is the elevator primarily for humans or machinery?”
            * Q2: “What is the total number of floors?”
            * Q3: “Is there a known budget constraint?”
2.2. Breadth-First Progression
1. Answer Top-Level Questions
    * The user sees Q1 in the Right-Hand Panel.
    * If a knowledge base is present, an “Auto-Populate” button might appear if the AI can infer an answer.
    * After Q1 is answered, Q1’s node on the canvas changes color to “answered,” and the system moves to Q2, then Q3.
2. Generating Child Nodes
    * If Q1 leads to follow-up questions, the AI generates children (Q4, Q5, etc.) that appear but remain locked until the system finishes the current set of siblings.
    * Once Q2 and Q3 are answered, the system goes back to each “parent question” that has children and proceeds to Q4, Q5, etc., in a breadth-first manner.
3. AI Logic at Each Step
    * The AI uses the user’s responses to decide which follow-ups to ask.
    * If conflicting info arises, it checks the conflict handling settings (auto vs. manual).
2.3. Conflict & Unknown Resolution (AI Assistance)
* Conflict Detection
    * Each time the user answers, the AI checks if it contradicts:
        * Previous answers,
        * The knowledge base.
    * If a minor conflict is detected and the user chose auto-resolve, the AI attempts to resolve it silently (e.g., by adjusting an earlier trivial detail).
    * Major conflicts trigger a prompt in the Right-Hand Panel, e.g.:“Conflict detected between Q5 and Q1. Please revise or confirm which one is correct.”
* Unknown Fields
    * If a user doesn’t provide needed data, the AI might guess if set to “Auto for trivial unknowns.”
    * Otherwise, it prompts again.
2.4. Revisiting & Editing Answers
* Clicking an Answered Node
    * On the Main Canvas, clicking Q2 highlights it in the Right-Hand Q&A Panel.
    * The user can change Q2’s answer, triggering the AI to reevaluate Q2’s child nodes.
    * If the new answer invalidates those children, the system either automatically discards them (if trivial) or prompts the user to confirm.
2.5. Stopping & Additional Inquiries
1. Max Questions Reached
    * If the user set a limit (e.g., 20), the system says:“You’ve hit 20 questions. Generate a mockup now, or continue?”
2. AI Decides
    * If no limit is set, the AI eventually says:“We likely have enough information for a design. Would you like to generate, or request more questions?”
3. User-Initiated Topics
    * At any time, the user can say:“I want more detail on accessibility features.”
    * The AI spawns new nodes (Q21, Q22) related to that.

Step 3: Consolidate & Generate
3.1. Requirements Document
* AI Compilation:
    * The AI gathers all answered questions (Q1–QX), along with system assumptions, auto-resolved conflicts, and AI “best guesses.”
* Preview & Download:
    * The user can preview the doc in-app.
    * Then choose to download (PDF, DOCX, or Markdown).
3.2. Prototype UI Code (React/Tailwind)
    * Code Generation by AI:
        * The AI (gpt-4-0125-preview) produces a sandboxed React + Tailwind (or HTML/CSS) mockup that reflects the collected requirements.
        * The user can interactively preview the mockup within the platform (e.g., a kiosk screen for elevator floor selection, zone definitions, accessibility controls, etc.).
    * Copy/Export:
        * A “Copy Code” button allows the user to paste the mockup’s code into their environment.
        * An “Export to Figma” option (via a plugin or generated tokens) can facilitate design iteration.
3.3. Multiple Versions
    * Generate Another Version:
        * If desired, the user can click a button to have the AI produce variations (e.g., different aesthetic, feature set, or layout).
        * Each version can be toggled via arrow buttons to compare side-by-side.

Interface Layout Summary
1. Main Canvas (Center)
    * Root Node (Prompt):
        * A single node labeled with the user’s prompt (e.g., “1000-floor elevator”) sits at the center—this is the trunk of the tree.
    * Top-Level Questions (Q1, Q2, Q3, etc.):
        * Each major question appears as a direct branch from the root node, forming the first layer of the tree.
    * Child Nodes (Q4, Q5, etc.):
        * When a parent question (e.g., Q1) leads to more detailed follow-ups, those sub-branches appear beneath it (Q4, Q5, etc.), creating additional levels in the tree. This can continue recursively to any depth needed.
    * Visual Indicators:
        * Conflict Icons: Mark major conflicts with an alert symbol, appearing on the affected node.
        * Auto-Filled Icons: If the AI uses knowledge base data to answer a question automatically, a special icon highlights that node.
2. Right-Hand Q&A Panel
    * Always open, showing the active question.
    * Displays AI-generated queries, conflict warnings (if manual resolution is required), and unknown prompts.
    * “Auto-Populate” button appears if a knowledge base is present and the AI has a confident guess.
3. Header / Toolbar (Top)
    * Steps: (1) Setup, (2) Q&A, (3) Generate.
    * A progress indicator, either “X / Y questions answered” or “AI confidence” meter.
    * A Knowledge Base status icon (uploaded or not).
4. Generation/Preview Area
    * After Q&A concludes, the user sees a Requirements Doc Preview and a React/Tailwind sandbox.
    * Tools for downloading the doc, copying code, or importing the UI design into other platforms.

Why This Flow Works
1. Root Prompt as Anchor
    * Keeps the design context clear throughout.
2. AI in Every Step
    * The gpt-4-0125-preview model handles question generation, unknown handling, conflict resolution (auto/manual), and final mockup/code creation.
3. Breadth-First Flow
    * Keeps siblings grouped, prevents the user from feeling overwhelmed by deep branching.
4. Minor Conflict Auto-Resolution
    * The AI can silently fix small contradictions, ensuring smoother progress.
5. Flexible Stopping
    * Either a specified question cap or at the AI’s discretion when it believes the design is sufficiently detailed.
6. Requirements & Live Prototype
    * Immediate translation of the Q&A results into a functional code sandbox and requirements doc.
7. Multiple Versions on Demand
    * Easy to iterate and compare alternative UI designs without losing prior answers.
